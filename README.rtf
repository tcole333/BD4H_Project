{\rtf1\ansi\ansicpg1252\cocoartf2580
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww13300\viewh9040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 README:\
\
-To run the code for this project pull the code down from the GitHub repository located at: https://github.com/tcole333/BD4H_Project/\
\
-Then create an environment using the requirements.txt file\
\
-The data for the project can be found on the CMS website and selecting a sample from the public use file: https://www.cms.gov/Research-Statistics-Data-and-Systems/Downloadable-Public-Use-Files/SynPUFs/DE_Syn_PUF\
\
-Processing.ipynb contains the code to run the ETL for the data to prepare it for decomposition and clustering and once the environment is set up and the data downloaded can be run with only modifications to the file paths for you local setup\
\
-After the ETL process has been run the Processing.ipynb can be run to generate the outputs from the decompositions and clustering for the tensors. We have also provided a sample of the tensor in df_sampled.npy that can be used directly in the Processing.ipynb without running the ETL steps locally after unzipping the file from GitHub.}